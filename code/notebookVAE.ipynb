{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "notebookVAE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Z5ZFAKAHAqU"
      },
      "source": [
        "# Colorizing Images using a VAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Vs6USKxHAqa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXJOIKdwHAqa"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTbvuqw_HIU9",
        "outputId": "9cd5008c-d22e-44b6-c9f3-a69114d1088f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Data directory\n",
        "dataDir = '/content/drive/MyDrive/Projects/gray2rgb'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOfVot5qHAqb"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import utils, datasets, transforms\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqrN4Tw7Kwkb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "a637377b-5050-4f6f-c41a-83848afe5e7a"
      },
      "source": [
        "'''# Run to extract a random validation set from training data\n",
        "import random\n",
        "\n",
        "files = os.listdir(os.path.join(dataDir,'trainData'))\n",
        "valFiles = random.sample(files,k=100)\n",
        "\n",
        "for f in valFiles:\n",
        "    os.rename(os.path.join(dataDir,'trainData',f), os.path.join(dataDir,'valData',f))\n",
        "'''"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"# Run to extract a random validation set from training data\\nimport random\\n\\nfiles = os.listdir(os.path.join(dataDir,'trainData'))\\nvalFiles = random.sample(files,k=100)\\n\\nfor f in valFiles:\\n    os.rename(os.path.join(dataDir,'trainData',f), os.path.join(dataDir,'valData',f))\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3dOGTdfHAqb"
      },
      "source": [
        "## Pre-processing to Generate Training Data\n",
        "\n",
        "I have a lot of colored images in my phone. I can convert them to grayscale and have enough input-output pairs for training the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVze13x2HAqb"
      },
      "source": [
        "# A custom dataset to generate (RGB, Grayscale) image pairs\n",
        "class myDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, path):\n",
        "        self.path = path\n",
        "        self.files = os.listdir(path)\n",
        "        self.transformIn = transforms.Compose([\n",
        "                     transforms.Resize(256),\n",
        "                     transforms.CenterCrop(256),\n",
        "                     transforms.ToTensor()])\n",
        "        self.transformOut = transforms.Compose([\n",
        "                     transforms.Grayscale(num_output_channels=1),\n",
        "                     transforms.Resize(256),\n",
        "                     transforms.CenterCrop(256),\n",
        "                     transforms.ToTensor()])\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        img = Image.open(os.path.join(self.path,self.files[index]))\n",
        "        inImgs = self.transformIn(img)\n",
        "        outImgs = self.transformOut(img)\n",
        "        return inImgs, outImgs\n",
        "\n",
        "# Initialized loader\n",
        "batchSize = 4\n",
        "\n",
        "dataset_train = myDataset(os.path.join(dataDir,'trainData'))\n",
        "loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batchSize, shuffle=True, num_workers=4, pin_memory=True)\n",
        "\n",
        "dataset_val = myDataset(os.path.join(dataDir,'valData'))\n",
        "loader_val = torch.utils.data.DataLoader(dataset_val, batch_size=batchSize, shuffle=True, num_workers=4, pin_memory=True)\n",
        "\n",
        "dataset_test = myDataset(os.path.join(dataDir,'testInput'))\n",
        "loader_test = torch.utils.data.DataLoader(dataset_test, batch_size=1, shuffle=False, num_workers=4)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xo34klkaHAqd"
      },
      "source": [
        "## Unet Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHoJfCl6HAqd"
      },
      "source": [
        "class encoder(nn.Module): # 1/2 of original size, 2 x channels\n",
        "    def __init__(self,input_channels):\n",
        "        super(encoder, self).__init__()\n",
        "        self.gen = nn.Sequential(\n",
        "                nn.Conv2d(input_channels,2*input_channels,kernel_size=3,stride=1,padding=1),\n",
        "                nn.BatchNorm2d(2*input_channels),\n",
        "                nn.LeakyReLU(0.2,inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.gen(x)\n",
        "\n",
        "class decoder(nn.Module): # 2 x size, 1/2 channel\n",
        "    def __init__(self, input_channels, skip_channels, final=False):\n",
        "        super(decoder, self).__init__()\n",
        "        self.gen1 = nn.Sequential(\n",
        "                nn.ConvTranspose2d(input_channels,input_channels//2,kernel_size=2,stride=2,padding=0),\n",
        "                nn.BatchNorm2d(input_channels//2),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(input_channels//2,input_channels//2,kernel_size=3,stride=1,padding=1),\n",
        "                nn.ReLU(inplace=True)\n",
        "        ) if not final else nn.Sequential(\n",
        "                nn.ConvTranspose2d(input_channels,input_channels,kernel_size=2,stride=2,padding=0),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(input_channels,input_channels,kernel_size=3,stride=1,padding=1),\n",
        "                nn.ReLU(inplace=True)\n",
        "        )\n",
        "        \n",
        "        self.gen2 = nn.Sequential(\n",
        "                nn.Conv2d(input_channels//2+skip_channels,input_channels//2,kernel_size=3,stride=1,padding=1),\n",
        "                nn.BatchNorm2d(input_channels//2),\n",
        "                nn.ReLU(inplace=True),\n",
        "        ) if not final else nn.Sequential(\n",
        "                nn.Conv2d(input_channels+skip_channels,input_channels,kernel_size=3,stride=1,padding=1),\n",
        "                nn.ReLU(inplace=True),\n",
        "        )\n",
        "        \n",
        "    def forward(self, x, xskip):\n",
        "        x = self.gen1(x)\n",
        "        x = self.gen2(torch.cat([x,xskip],axis=1))\n",
        "        return x\n",
        "    \n",
        "class Unet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Unet, self).__init__()\n",
        "        self.e1 = encoder(1) # 256,1 -> 128,2\n",
        "        self.e2 = encoder(2) # 128,2 -> 64,4\n",
        "        self.e3 = encoder(4) # 64,4 -> 32,8\n",
        "        self.e4 = encoder(8) # 32,8 -> 16,16 \n",
        "        self.e5 = encoder(16) # 16,16 -> 8,32\n",
        "        self.e6 = encoder(32) # 8,32 -> 4,64\n",
        "        self.e7 = encoder(64) # 4,64 -> 2,128\n",
        "        self.e8 = encoder(128) # 2,128 -> 1,256\n",
        "        self.l1 = nn.Sequential(\n",
        "                  nn.Linear(256,256),\n",
        "                  nn.ReLU(inplace=True),\n",
        "                  nn.Linear(256,256),\n",
        "                  nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.d1 = decoder(256,128) # 1,256 -> 2,128 .. 2,128 -> 2,256 -> 2,128\n",
        "        self.d2 = decoder(128,64) # 2,128 -> 4,64 .. 4,64 -> 4,128 -> 4,64\n",
        "        self.d3 = decoder(64,32) # 4,64 -> 8,32 .. 8,32 -> 8,64 -> 8,32\n",
        "        self.d4 = decoder(32,16) # 8,32 -> 16,16 .. 16,16 -> 16,32 -> 16,16\n",
        "        self.d5 = decoder(16,8) # 16,16 -> 32,8 .. 32,8 -> 32,16 -> 32,8\n",
        "        self.d6 = decoder(8,4) # 32,8 -> 64,4 .. 64,4 -> 64,8 -> 64,4\n",
        "        self.d7 = decoder(4,2, final=True) # 64,4 -> 128,4 .. 128,2 -> 128,6 -> 128,4\n",
        "        self.d8 = decoder(4,1, final=True) # 128,4 -> 256,4 .. 256,1 -> 256,5 -> 256,4\n",
        "        self.last = nn.Sequential(\n",
        "            nn.Conv2d(4, 3, kernel_size=3, stride=1, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        ) # 256,4 -> 256,3\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x1 = self.e1(x)\n",
        "        x2 = self.e2(x1)\n",
        "        x3 = self.e3(x2)\n",
        "        x4 = self.e4(x3)\n",
        "        x5 = self.e5(x4)\n",
        "        x6 = self.e6(x5)\n",
        "        x7 = self.e7(x6)\n",
        "        x8 = self.e8(x7)\n",
        "        \n",
        "        x81 = x8.view(-1,256)\n",
        "        x82 = self.l1(x81)\n",
        "        x83 = x82.view(-1,256,1,1)\n",
        "\n",
        "        x9 = self.d1(x83, x7)\n",
        "        x10 = self.d2(x9, x6)\n",
        "        x11 = self.d3(x10, x5)\n",
        "        x12 = self.d4(x11, x4)\n",
        "        x13 = self.d5(x12, x3)\n",
        "        x14 = self.d6(x13, x2)\n",
        "        x15 = self.d7(x14, x1)\n",
        "        x16 = self.d8(x15, x)\n",
        "        output = self.last(x16)\n",
        "        \n",
        "        return output"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AZYZ_CoHAqe"
      },
      "source": [
        "### Helper Functions for Validation and Testing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eitCBYVkR0c3"
      },
      "source": [
        "def valRun(model, loader, device):\n",
        "    rgbs, grays = next(iter(loader))\n",
        "    preds = model(grays.to(device))\n",
        "\n",
        "    graygrd = utils.make_grid(grays)\n",
        "    rgbgrd = utils.make_grid(rgbs)\n",
        "    predgrd = utils.make_grid(preds.detach().cpu())\n",
        "\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(3,1)\n",
        "    ax1.imshow(graygrd.permute(1,2,0))\n",
        "    ax2.imshow(rgbgrd.permute(1,2,0))\n",
        "    ax3.imshow(predgrd.permute(1,2,0))\n",
        "\n",
        "    for ax in [ax1, ax2, ax3]:\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    plt.show()\n",
        "    time.sleep(1)\n",
        "    return\n",
        "\n",
        "def testRun(model, loader, device):\n",
        "    for i, (_, grays) in enumerate(loader):\n",
        "        preds = model(grays.to(device))\n",
        "\n",
        "        predgrd = utils.make_grid(preds.detach().cpu())\n",
        "\n",
        "        toImg = transforms.ToPILImage()\n",
        "        img = toImg(predgrd)\n",
        "        img.save(os.path.join(dataDir,'testOutput',f'{i}.jpeg'))\n",
        "    return \n",
        "\n",
        "def saveChkPt(state, filename):\n",
        "    torch.save(state,filename)\n",
        "    return\n",
        "\n",
        "def loadChkPt(filename, model, optimizer=None):\n",
        "    chkpt = torch.load(filename)\n",
        "    model.load_state_dict(chkpt['model'])\n",
        "    if optimizer!=None: optimizer.load_state_dict(chkpt['optimizer'])\n",
        "    loss_train = chkpt['loss_train']\n",
        "    loss_val = chkpt['loss_val']\n",
        "    return model, optimizer, chkpt['epoch'], loss_train, loss_val"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiQtpC4GHAqf"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MarM-YkXHAqf",
        "outputId": "8d72cbcb-b0ae-4a17-c799-9709e30fbd99"
      },
      "source": [
        "nEpochs = 50\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "step_interval = 1000\n",
        "save_interval = 10\n",
        "epoch0 = 20\n",
        "\n",
        "model = Unet().to(device)\n",
        "reconCriterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "loss_train = []\n",
        "loss_val = []\n",
        "if epoch0>0:\n",
        "    model, optimizer, _, loss_train, loss_val = loadChkPt(os.path.join(dataDir,'checkpoints',f'chkpt_{epoch0-1}.pt'), model, optimizer)\n",
        "\n",
        "cur_step=0\n",
        "\n",
        "for epoch in range(nEpochs):\n",
        "    print(f'Epoch: {epoch0+epoch}')\n",
        "    loss_train_s = 0\n",
        "    loss_val_s = 0\n",
        "    \n",
        "    # Train\n",
        "    model.train()\n",
        "    for i, (rgbs, grays) in enumerate(loader_train):\n",
        "        rgbs = rgbs.to(device)\n",
        "        grays = grays.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        pred = model(grays)\n",
        "        loss = reconCriterion(pred, rgbs)\n",
        "        loss_train_s = loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        cur_step+=1        \n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (rgbs, grays) in enumerate(loader_val):\n",
        "            rgbs = rgbs.to(device)\n",
        "            grays = grays.to(device)\n",
        "            pred = model(grays)\n",
        "            loss = reconCriterion(pred, rgbs)\n",
        "            loss_val_s+=loss.item()\n",
        "\n",
        "    loss_train.append(loss_train_s)\n",
        "    loss_val.append(loss_val_s/len(loader_val))\n",
        "\n",
        "    if (epoch0+epoch+1)%save_interval==0:\n",
        "        chkpt = {'epoch': epoch0+epoch,\n",
        "                 'model': model.state_dict(),\n",
        "                 'optimizer': optimizer.state_dict(),\n",
        "                 'loss_train': loss_train,\n",
        "                 'loss_val': loss_val}\n",
        "        saveChkPt(chkpt, os.path.join(dataDir,'checkpoints',f'chkpt_{epoch0+epoch}.pt'))\n",
        "\n",
        "        print(f'Steps: {cur_step}, Train Loss: {loss_train[-1]}, Val Loss: {loss_val[-1]}')\n",
        "        valRun(model,loader_val,device)\n",
        "\n",
        "        plt.figure()\n",
        "        plt.plot(range(epoch0+epoch+1), loss_train, label='Train')\n",
        "        plt.plot(range(epoch0+epoch+1), loss_val, label='Validation')\n",
        "        plt.legend()\n",
        "        plt.ylabel('Cost')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.show()\n",
        "        time.sleep(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 20\n",
            "Epoch: 21\n",
            "Epoch: 22\n",
            "Epoch: 23\n",
            "Epoch: 24\n",
            "Epoch: 25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kq2ZatOBz7kT"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvhy3REqdn_e"
      },
      "source": [
        "epoch0 = 50\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "model = Unet().to(device)\n",
        "if epoch0>0:\n",
        "    model, _, _, _, _ = loadChkPt(os.path.join(dataDir,'checkpoints',f'chkpt_{epoch0-1}.pt'), model)\n",
        "\n",
        "testRun(model, loader_test, device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeHyuUPCcqam"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}